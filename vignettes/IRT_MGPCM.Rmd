---
title: "Bayesian Evaluation of a Generalized Partial Credit Model Using the bleval Package"
author: "Jieyuan Dong"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

## Introduction

This document demonstrates the evaluation of a Bayesian Generalized Partial Credit Model (GPCM) using the `bleval` package. The model includes five latent variables (personality traits). We illustrate the workflow from data preparation to model evaluation, including parameter estimation in CmdStan, convergence diagnostics, and computation of information criteria (DIC, WAIC, LOOIC) and fully marginal likelihood using the `bleval` package.

## Read Data

We begin by loading the `bleval` package and the dataset `example2_GPCM`, which contains the observed responses for the GPCM.

```{r}
# Install the bleval package
devtools::install_github("luoxh3/bleval")

# Load the package
library(bleval)

# Load the dataset from the bleval package
data(example2_GPCM, package = "bleval")

D <- 5  # number of traits (number of dimensions)
K <- 5  # number of categories
I <- nrow(example2_GPCM)  # number of respondents (i.e. units)
J <- 20  # number of items

res <- as.matrix(example2_GPCM)

# Set the item-trait indicator
it <- rep(1:D, each=J/D)
  
# Set the item-wording indicator
iw <- rep(1, J)
iw[grep("_R", names(example2_GPCM))] <- -1
```

## Parameter Estimation in CmdStan

We define the Bayesian GPCM in CmdStan and estimate parameters using MCMC sampling.

```{r}
library(cmdstanr)
library(posterior)

stan_code <- "
functions {  // function for GPCM
  real gpcm(int y, real theta, real alpha, vector beta) {
    vector[rows(beta) + 1] unsummed;
    vector[rows(beta) + 1] probs;
    unsummed = append_row(rep_vector(0.0, 1), alpha*(theta - beta));
    probs = softmax(cumulative_sum(unsummed));
    return categorical_lpmf(y | probs);
  }
}
data {
  int<lower=1> D;                      // number of traits
  int<lower=1> I;                      // number of respondents
  int<lower=1> J;                      // number of items
  int<lower=1> N;                      // number of responses
  int<lower=2> K;		                   // number of response categories
  array[J] int<lower=1,upper=D> it;    // item-trait indicator
  array[J] int<lower=-1,upper=1> iw;   // item-wording indicator
  array[N] int<lower=1,upper=I> ii;    // person id
  array[N] int<lower=1,upper=J> jj;    // item id
  array[N] int<lower=1,upper=K> y;     // responses
}
parameters {
  vector<lower=0>[J] alpha;            // item discrimination parameters
  array[J] vector[K-1] beta;		       // item step difficulties
  corr_matrix[D] Omega;                // correlation matrix of traits
  array[I] vector[D] theta;		         // traits
}
transformed parameters{
  cov_matrix[D] Sigma;                 // covariance matrix of traits
  Sigma = quad_form_diag(Omega, rep_vector(1.0, D));
}
model {
  // prior
  alpha ~ lognormal(0, 1);
  for (j in 1:J)
    beta[j] ~ normal(0, 10);
  Omega ~ lkj_corr(1);
  theta ~ multi_normal(rep_vector(0, D), Sigma);

  // likelihood
  for (n in 1:N)
    target += gpcm(y[n],
      theta[ii[n], it[jj[n]]], iw[jj[n]]*alpha[jj[n]], beta[jj[n]]);
}
"

data_list <- list(
  D = D,
  K = K,
  I = I,
  J = J,
  N = I*J,
  iw = iw,
  it = it,
  ii = rep(1:I, each = J),
  jj = rep(1:J, times = I),
  y = as.vector(t(res))
)

# Compile the CmdStan model
model_file <- write_stan_file(stan_code,  dir = getwd(), basename = "GPCM")
stan_model <- cmdstan_model(model_file)

# Run the No-U-Turn Sampler with a burn-in period of 2500 iterations
fit <- stan_model$sample(
  data = data_list,
  seed = 1,
  refresh = 500,          
  chains = 4, 
  parallel_chains = 4,
  iter_warmup = 2500,
  iter_sampling = 2500,
  thin = 1,
  show_messages = TRUE
)

# Draw posterior samples from the model
draws <- fit$draws(format="df")

# Extract posterior samples for model parameters only (excluding latent variables)
# For the correlation matrix only the lower triangular elements are extracted
par_cols <- grep("^alpha|^beta", names(draws), value = TRUE)
par <- as.matrix(draws[, par_cols])

Omega_elements <- grep("^Omega", names(draws), value = TRUE)
Omega_lower_tri <- as.matrix(draws[, Omega_elements[lower.tri(diag(D))]])

samples <- cbind(par, Omega_lower_tri)

# Extract posterior samples for latent variables
lv_draws <- as.matrix(draws[, grep("^Theta", colnames(draws))])
```

## Model Evaluation with `bleval`

### Compute Information Criteria

**Step 1: Specify the `log_joint_i` function**

The `log_joint_i` function calculates the log joint density for each unit.

```{r}
log_joint_i <- function(samples_s, data, i, Ngrid, nodes) {
  
  # conditional likelihood
  D <- data[["D"]]
  J <- data[["J"]]
  K <- data[["K"]]    
  it <- data[["it"]]
  iw <- data[["iw"]]
  y_i <- data[["y"]][data[["ii"]]==i]
  jj_i <- data.frame(jj=data[["jj"]][data[["ii"]]==i])
  
  alpha <- samples_s[grep("^alpha", names(samples_s), value = TRUE)]
  beta <- matrix(samples_s[grep("^beta", names(samples_s), value = TRUE)],
                 J, K - 1)
  Omega_lower_tri <- samples_s[grep("^Omega", names(samples_s), value = TRUE)]
  Omega <- matrix(0, D, D)
  Omega[lower.tri(Omega)] <- Omega_lower_tri
  Omega <- Omega + t(Omega) + diag(D)

  Mu <- rep(0, D)
  sigma <- rep(1, D)
  Sigma <- diag(sigma) %*% Omega %*% diag(sigma)
  
  Nnode <- nrow(nodes)
  
  log_con_i_j <- apply(jj_i, 1, function(j){
    int <- cumsum(c(0, -beta[j,]))
    Int <- t(replicate(Nnode, int))
    scoring <- t(replicate(Nnode, 0:(K-1)))
    numerator <- exp(iw[j] * alpha[j] * (replicate(K, nodes[, it[j]])*scoring + Int))
    denominator <- rowSums(numerator)
    cat_prob <- numerator/denominator
    
    extraDistr::dcat(y_i[j], cat_prob, log=TRUE)
  }) # conditional density given respondent i and item j
  
  if(is.vector(log_con_i_j))
    log_con_i <- sum(log_con_i_j) else 
    log_con_i <- rowSums(log_con_i_j)
    
  # log prior density of latent variables
  log_lv_i <- mvtnorm::dmvnorm(nodes, mean=Mu, sigma=Sigma, log = TRUE)
  
  # return
  log_lv_i + log_con_i
}
```

**Step 2: Compute the posterior means and covariance matrices of latent variables**

```{r}
# Create lists to store posterior means and covariance matrices of latent traits
lv_list <- list()
for (i in 1:I){
  lv_list[[i]] <- lv_draws[ ,seq(i, (i + (D-1)*I), I)]
}

# Compute the posterior means of the latent traits for unit i
lv_mu_list <- lapply(lv_list, colMeans)
# Compute the posterior covariance matrices of the latent traits for unit i
lv_cov_list <- lapply(lv_list, cov)

# Make sure that N denotes the number of units (i.e. respondents) in the data to be passed to the functions below
data_list$N <- data_list$I
```

**Step 3: Compute the log marginal likelihood integrating out latent variables**

The `log_marglik` function from the `bleval` package computes the log marginal likelihood by integrating out the latent variables using adaptive Gauss-Hermite quadrature.

```{r}
log_marglik_result <- bleval::log_marglik(samples = samples, data = data_list, Ngrid = 3,
                                          lv_mu = lv_mu_list, lv_cov = lv_cov_list,
                                          log_joint_i = log_joint_i, n_cores = 50)
```

**Step 4: Compute information criteria**

The `calc_IC` function from the `bleval` package computes the information criteria (DIC, WAIC, and LOOIC) based on the log marginal likelihoods obtained from the `log_marglik` function.

```{r}
bleval::calc_IC(log_marglik_result, 1)
```

### Compute Fully Marginal Likelihood

**Step 1: Specify the `log_prior` function**

The `log_prior` function calculates the log prior density for model parameters. This function takes a named vector of parameter values from a posterior sample and returns the log prior density for each parameter.

```{r}
log_prior <- function(samples_s) {
  
  alpha <- samples_s[grep("^alpha", names(samples_s), value = TRUE)]
  beta <- samples_s[grep("^beta", names(samples_s), value = TRUE)]

  D <- 5
  Omega_lower_tri <- samples_s[grep("^Omega", names(samples_s), value = TRUE)]
  Omega <- matrix(0, D, D)
  Omega[lower.tri(Omega)] <- Omega_lower_tri
  Omega <- Omega + t(Omega) + diag(D)

  do_lkj_constant <- function(eta, K) {
    Km1 <- K - 1
    if (eta == 1) {
      denominator_length <- floor(Km1 / 2)
      denominator <- numeric(denominator_length)
      for (k in 1:denominator_length) {
        denominator[k] <- lgamma(2 * k)
      }
      constant <- -sum(denominator)
      
      if (K %% 2 == 1) {
        term1 <- 0.25 * (K^2 - 1) * log(pi)
        term2 <- 0.25 * (Km1^2) * log(2)
        term3 <- Km1 * lgamma(0.5 * (K + 1))
        constant <- constant - (term1 - term2 - term3)
      } else {
        term1 <- 0.25 * K * (K - 2) * log(pi)
        term2 <- 0.25 * (3*K^2 - 4*K) * log(2)
        term3 <- K * lgamma(0.5 * K)
        term4 <- Km1 * lgamma(K)
        constant <- constant - (term1 + term2 + term3 - term4)
      }
    } else {
      constant <- Km1 * lgamma(eta + 0.5 * Km1)
      for (k in 1:Km1) {
        term1 <- 0.5 * k * log(pi)
        term2 <- lgamma(eta + 0.5 * (Km1 - k))
        constant <- constant - (term1 + term2)
      }
    }
    return(constant)
  }

  lkj_corr_lpdf <- function(y, eta) {
    K <- nrow(y)

    constant <- do_lkj_constant(eta, K)
    
    if (eta == 1) return(constant)

    # sum_values = log(det(y))
    sum_values <- determinant(y, logarithm=TRUE)$modulus[1]
  
    # lpdf = normalizing constant + (eta-1)*log(det(y))
    lp <- constant + (eta - 1) * sum_values
    return(lp)
  } # obtain the log probability density of LKJ correlation distribution

  sum(dlnorm(alpha, meanlog = 0, sdlog = 1, log = TRUE)) +
    sum(dnorm(beta, mean = 0, sd = 10, log = TRUE)) +
    lkj_corr_lpdf(Omega, eta = 1)
}
```

**Step 2: Define parameter bounds**

The `log_fmarglik` function requires lower and upper bounds for the model parameters.

```{r}
lb <- c(rep(0, J), rep(-Inf, J*(K-1)), rep(-Inf, ncol(L_lower_tri)))
ub <- c(rep(Inf, J), rep(Inf, J*(K-1)), rep(Inf, ncol(L_lower_tri)))

names(lb) <- names(ub) <- colnames(samples)
```

**Step 3: Compute the log fully marginal likelihood integrating out both latent variables and model parameters**

The `log_fmarglik` function from the `bleval` package computes the log fully marginal likelihood by integrating out both the latent variables and the model parameters.

```{r}
bleval::log_fmarglik(samples = samples, data = data_list, Ngrid = 3, 
                     lv_mu = lv_mu_list, lv_cov = lv_cov_list, 
                     log_joint_i = log_joint_i, log_prior = log_prior, 
                     lb = lb, ub = ub,
                     cores = 50  # the number of cores used for evaluating log_posterior
                     # an argument passed to the function bridge_sampler()
                     )
```

The output of this function is the log fully marginal likelihood, which can be used to compute Bayes factors for model comparison.
